{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e7feb5a2",
      "metadata": {
        "id": "e7feb5a2"
      },
      "source": [
        "# WS 12 AutoML with AutoGluon Hands on Module"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this hands on module, we will see how to simplify the process of training high-quality, optimized machine learning models on sample datasets from UCI Machine Learning repository using the [AutoGluon](https://auto.gluon.ai/stable/index.html) package.\n",
        "We start by installing the `utogluon` and `ucimlrepo` packages with `pip`"
      ],
      "metadata": {
        "id": "gKRURZHj11-T"
      },
      "id": "gKRURZHj11-T"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autogluon\n",
        "!pip install ucimlrepo"
      ],
      "metadata": {
        "id": "Dn8m61eN2BSJ"
      },
      "id": "Dn8m61eN2BSJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we import pacakges and load in the following three healthcare related datasets from [UCI Machine Learning Repository](https://archive.ics.uci.edu/)\n",
        "\n",
        "\n",
        "*   [Heart Disease data](https://archive.ics.uci.edu/dataset/45/heart+disease)\n",
        "*   [Breast Cancer data](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic) from University of Wisconsin\n",
        "*   [Diabetes data](https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) representing ten years of clinical care at 130 US hospitals\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NiMElLZy24Zn"
      },
      "id": "NiMElLZy24Zn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b62dd5b",
      "metadata": {
        "id": "2b62dd5b"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ucimlrepo import fetch_ucirepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cff198e",
      "metadata": {
        "id": "4cff198e"
      },
      "outputs": [],
      "source": [
        "# load in the heart disease dataset from UCI\n",
        "heart_disease = fetch_ucirepo(id=45)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = heart_disease.data.features\n",
        "y = heart_disease.data.targets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# variable information\n",
        "print(heart_disease.variables)"
      ],
      "metadata": {
        "id": "Y78H633V4McN"
      },
      "id": "Y78H633V4McN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heart disease dataset uses a multi-class label with integer values ranging from 0 - 4 with the following semantics\n",
        "\n",
        "\n",
        "*   0 (no heart disease)\n",
        "*   1-4 (increasing severity of hear disease)\n",
        "\n",
        "In order to simplify the classification problem, we convert these to binary labels, with 0/1 indicating absence/presence of heart disease\n",
        "\n"
      ],
      "metadata": {
        "id": "KlWuL1tfzizR"
      },
      "id": "KlWuL1tfzizR"
    },
    {
      "cell_type": "code",
      "source": [
        "# finalize the heart disease dataset in a single DataFrame with predictors and labels\n",
        "heart_disease_df = X.assign(\n",
        "    binary_label=y.map(lambda value: value > 0).astype(int) # convert categorical labels to binary (1=heart disease, 0=no heart disease)\n",
        ")"
      ],
      "metadata": {
        "id": "YqvU2QZD3uL2"
      },
      "id": "YqvU2QZD3uL2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heart_disease_df"
      ],
      "metadata": {
        "id": "wNyknnrn-EmA"
      },
      "id": "wNyknnrn-EmA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heart_disease_df['binary_label'].value_counts()"
      ],
      "metadata": {
        "id": "wgscPRdW-HA-"
      },
      "id": "wgscPRdW-HA-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we load in the breast cancer dataset. This dataset contains features that describe the characteristics of cell nuclei present in a digitized image taken from the fine needle aspirate of a breast mass. The labels in the data are binary/two-class, with 'B' representing a benign mass and 'M' representing a malignant mass\n",
        "\n"
      ],
      "metadata": {
        "id": "oa0rkUeg5JDh"
      },
      "id": "oa0rkUeg5JDh"
    },
    {
      "cell_type": "code",
      "source": [
        "# now we load in the breast cancer dataset from UCI\n",
        "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = breast_cancer_wisconsin_diagnostic.data.features\n",
        "y = breast_cancer_wisconsin_diagnostic.data.targets\n"
      ],
      "metadata": {
        "id": "eNnlKxMf43u9"
      },
      "id": "eNnlKxMf43u9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(breast_cancer_wisconsin_diagnostic.variables)"
      ],
      "metadata": {
        "id": "PT0a0HTa6U-B"
      },
      "id": "PT0a0HTa6U-B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "breast_cancer_df = X.assign(\n",
        "    Diagnosis=y\n",
        ")"
      ],
      "metadata": {
        "id": "vuhB5yTh91__"
      },
      "id": "vuhB5yTh91__",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "breast_cancer_df"
      ],
      "metadata": {
        "id": "I9tbb7hb99Ni"
      },
      "id": "I9tbb7hb99Ni",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "breast_cancer_df['Diagnosis'].value_counts()"
      ],
      "metadata": {
        "id": "5eKQ2XK0-MWe"
      },
      "id": "5eKQ2XK0-MWe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we load in the Diabetes dataset. This dataset was constructed with the goal of predicting the early readmission of diabetes patients within 30 days of discharge"
      ],
      "metadata": {
        "id": "VTKvVtP3QstW"
      },
      "id": "VTKvVtP3QstW"
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch dataset\n",
        "diabetes_data = fetch_ucirepo(id=296)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = diabetes_data.data.features\n",
        "y = diabetes_data.data.targets"
      ],
      "metadata": {
        "id": "c3nILN1VRAYE"
      },
      "id": "c3nILN1VRAYE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(diabetes_data.variables)"
      ],
      "metadata": {
        "id": "x4y6ROy8RG9q"
      },
      "id": "x4y6ROy8RG9q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_df = X.assign(\n",
        "    readmitted=y\n",
        ")"
      ],
      "metadata": {
        "id": "JKzx07jxRMhH"
      },
      "id": "JKzx07jxRMhH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_df['readmitted'].value_counts()"
      ],
      "metadata": {
        "id": "Oe1LpCMARRBq"
      },
      "id": "Oe1LpCMARRBq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we split the two datasets into 80%/20% training/test set splits, so that we can evaluate our tuned models at the very end on unseen test data"
      ],
      "metadata": {
        "id": "PciRQPzH639S"
      },
      "id": "PciRQPzH639S"
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the heart disease dataset into training and test sets using DataFrame.sample()\n",
        "hd_train = heart_disease_df.sample(frac=0.8)\n",
        "hd_test = heart_disease_df.drop(hd_train.index)"
      ],
      "metadata": {
        "id": "WPe-3rr867cu"
      },
      "id": "WPe-3rr867cu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hd_train['binary_label'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "uL8bDStN8zsD"
      },
      "id": "uL8bDStN8zsD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hd_test['binary_label'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "oUWhfJy082Zv"
      },
      "id": "oUWhfJy082Zv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bc_train = breast_cancer_df.sample(frac=0.8)\n",
        "bc_test = breast_cancer_df.drop(bc_train.index)"
      ],
      "metadata": {
        "id": "MDdZzW2OSvmO"
      },
      "id": "MDdZzW2OSvmO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bc_train['Diagnosis'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "N7UATHpSS3Og"
      },
      "id": "N7UATHpSS3Og",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bc_test['Diagnosis'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "wA7YrbzcS5-v"
      },
      "id": "wA7YrbzcS5-v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_binary = diabetes_df.assign(\n",
        "    binary_label=lambda x: x['readmitted'].map(lambda label: 1 if label == '<30' else 0)\n",
        ").drop(columns='readmitted')\n",
        "diabetes_train = diabetes_binary.sample(frac=0.8)\n",
        "diabetes_test = diabetes_binary.drop(diabetes_train.index)"
      ],
      "metadata": {
        "id": "UhxSy-bTTGJi"
      },
      "id": "UhxSy-bTTGJi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_train['binary_label'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "FNIVuqVDTOBR"
      },
      "id": "FNIVuqVDTOBR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_test['binary_label'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "sCAS2hIwTQcf"
      },
      "id": "sCAS2hIwTQcf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, for illustrative purposes, we see how much code it would take to implement a similar *(highly simplified)* k-fold bagging + stacking model ensembling such as what AutoGluon does automatically using Scikit Learn, another popular machine learning framework for Python"
      ],
      "metadata": {
        "id": "BFD5f5iOVmt9"
      },
      "id": "BFD5f5iOVmt9"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from xgboost import XGBClassifier\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "7rHfS2Z7V9Tm"
      },
      "id": "7rHfS2Z7V9Tm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: In newer versions of scikit learn >= 1.4, the RandomForest classifier can handle NaNs\n",
        "# This saves us from having to impute them explicitly, but normally we would have to deal with this\n",
        "heart_disease_df"
      ],
      "metadata": {
        "id": "m1ENeFkuWe0k"
      },
      "id": "m1ENeFkuWe0k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separate features X from targets y\n",
        "X = hd_train.drop(columns=['binary_label'])\n",
        "y = hd_train['binary_label']\n",
        "# initialize the Kfold object for doing kfold cross validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# construct arrays for storing the out of fold prediciotns for the models\n",
        "oof_preds_rf = np.zeros(len(X))\n",
        "oof_preds_xgb = np.zeros(len(X))\n",
        "\n",
        "# save the bagged models in lists\n",
        "\n",
        "# specifiy the classifiers that will be in each layer\n",
        "layers = [RandomForestClassifier, XGBClassifier]\n",
        "layer_preds = [oof_preds_rf, oof_preds_xgb]\n",
        "layer_bags = [list(), list()]\n",
        "# loop over our layers\n",
        "for i, layer in enumerate(layers):\n",
        "  print(f\"Performing k-fold cross validation at layer {i} with {layer}\")\n",
        "  # do the K-fold cross validation loop\n",
        "  for train_idx, val_idx in tqdm(kf.split(X), total=5):\n",
        "      # split inputs and outputs into training and validation\n",
        "      X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "      y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "      # if we are in a layer past the first layer, the inputs need to include\n",
        "      # the predictions from the prior layer\n",
        "      if i > 0:\n",
        "        X_train = np.column_stack([\n",
        "            X_train.to_numpy(),\n",
        "            layer_preds[i-1][train_idx] # include preds from prior layer\n",
        "        ])\n",
        "        X_val = np.column_stack([\n",
        "            X_val.to_numpy(),\n",
        "            layer_preds[i-1][val_idx]\n",
        "        ])\n",
        "\n",
        "      # fit a model from the given layer on the training fold\n",
        "      model = layer()\n",
        "      model.fit(X_train, y_train)\n",
        "      # evaluate it on the validation fold and save oof predictions\n",
        "      layer_preds[i][val_idx] = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "      # save the model in our layer bag\n",
        "      layer_bags[i].append(model)\n",
        "\n",
        "# final meta model: Weighted ensemble of the predictions from the prior layers\n",
        "meta_features = np.column_stack(layer_preds)\n",
        "\n",
        "meta_model = LogisticRegression()\n",
        "meta_model.fit(meta_features, y)"
      ],
      "metadata": {
        "id": "tgH2TZ5PWHfD"
      },
      "id": "tgH2TZ5PWHfD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to evaluate this custom Meta model on the unseen test data, we need to\n",
        "\n",
        "\n",
        "1.   Get 5 separate sets of predictions from each Random Forest model in the first layer\n",
        "2.   Get 5 separate sets of predictions from each XGBoost model in the second layer, appending the predictions from the first layer models as feature inputs to the second layer models\n",
        "3. Average the predicions made at each layer into a single set of predictions per layer\n",
        "4. Append these two sets of predictions together into the final features to feed to the Meta Model (Logistic Regression)\n",
        "5. Get the final predictions from the Meta Model\n",
        "\n"
      ],
      "metadata": {
        "id": "74LPiyADbaz7"
      },
      "id": "74LPiyADbaz7"
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = hd_test.drop(columns=['binary_label'])\n",
        "y_test = hd_test['binary_label']\n",
        "\n",
        "layer_preds_test = []\n",
        "\n",
        "# steps 1 and 2 - Getting separate predictions from models in each layer\n",
        "for i in range(len(layer_bags)):\n",
        "  # concatenate predictions from prior later to features if needed\n",
        "  if i > 0:\n",
        "    X_test_stacked = np.column_stack([X_test.to_numpy(), layer_preds_test[i-1]])\n",
        "  else:\n",
        "    X_test_stacked = X_test\n",
        "\n",
        "  # step 3 - compute average predictions across all models in the layer\n",
        "  current_layer_preds = sum([\n",
        "      model.predict_proba(X_test_stacked)[:, 1]\n",
        "      for model in layer_bags[i]\n",
        "  ]) / len(layer_bags[i])\n",
        "\n",
        "  # save the predictions for the layer\n",
        "  layer_preds_test.append(current_layer_preds)\n",
        "\n",
        "# Step 4 - concatenate layer predicitons\n",
        "meta_features_test = np.column_stack(layer_preds_test)\n",
        "\n",
        "# Step 5 - get final predictions\n",
        "final_predictions = meta_model.predict(meta_features_test)\n",
        "\n",
        "# get the accuracy and f1 score\n",
        "print(f\"F1: {f1_score(y_test, final_predictions)}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, final_predictions)}\")\n",
        ""
      ],
      "metadata": {
        "id": "m5xTfZBVbBE8"
      },
      "id": "m5xTfZBVbBE8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that even to try and capture a minimal representation of what AutoGluon is implementing for us under the hood at training and inference time, it took a considerable amount of code, understanding, and index manipulation, and we didn't even implement greedy weighting and model pruning. \\\n",
        "Now we will see how AutoGluon's `TabularPredictor` class can be used to automatically fit a weighted ensemble on the same dataset, with automatic K-fold cross validation, bagging, and stacking, with a much larger suite of models evaluated for inclusion in the final ensemble."
      ],
      "metadata": {
        "id": "09Bv3a8n8ZsE"
      },
      "id": "09Bv3a8n8ZsE"
    },
    {
      "cell_type": "code",
      "source": [
        "from autogluon.tabular import TabularPredictor"
      ],
      "metadata": {
        "id": "SlqyUWsF8nje"
      },
      "id": "SlqyUWsF8nje",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting a tabular predictor on the Heart Disease Dataset\n",
        "predictor_hd = TabularPredictor( # construct the predictor\n",
        "    label='binary_label', eval_metric='roc_auc'\n",
        ").fit( # call the fit method\n",
        "    hd_train,\n",
        "    num_bag_folds=3 # perform k-fold cross validation\n",
        ")"
      ],
      "metadata": {
        "id": "ZVdjiVDX8k0g"
      },
      "id": "ZVdjiVDX8k0g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we evaluate the AutoGluon model on the test dataset, and also show a leaderboard with a performance breakdown across all models trained during construction of the ensemble"
      ],
      "metadata": {
        "id": "SK_ggR0wCSDZ"
      },
      "id": "SK_ggR0wCSDZ"
    },
    {
      "cell_type": "code",
      "source": [
        "predictor_hd.evaluate(hd_test)"
      ],
      "metadata": {
        "id": "d18jJcAnCY9x"
      },
      "id": "d18jJcAnCY9x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor_hd.leaderboard(hd_test)"
      ],
      "metadata": {
        "id": "wmgG1ebZmncH"
      },
      "id": "wmgG1ebZmncH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bc_train"
      ],
      "metadata": {
        "id": "-HFSh7cPnuUe"
      },
      "id": "-HFSh7cPnuUe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we train another TabularPredictor on the BreastCancer data, showing that AutoGluon can handle Categorical targets out of the box, without the requirement to manually encode targets as integers/one-hot"
      ],
      "metadata": {
        "id": "i_3rXJganwrt"
      },
      "id": "i_3rXJganwrt"
    },
    {
      "cell_type": "code",
      "source": [
        "# here we do the same with the breast cancer dataset\n",
        "predictor_bc = TabularPredictor( # construct the predictor\n",
        "    label='Diagnosis', eval_metric='roc_auc'\n",
        ").fit( # call the fit method\n",
        "    bc_train\n",
        ")"
      ],
      "metadata": {
        "id": "8gl2x1gFni7z"
      },
      "id": "8gl2x1gFni7z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor_bc.evaluate(bc_test)"
      ],
      "metadata": {
        "id": "OLSs2U7an9TW"
      },
      "id": "OLSs2U7an9TW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor_bc.leaderboard(bc_test)"
      ],
      "metadata": {
        "id": "1N7OTge1oRxT"
      },
      "id": "1N7OTge1oRxT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that AutoGluon's Tabular Predictor achieves performance competitive with that of state of the art baseline models with minimal code and Machine Learning engineering understanding. However, let's assess how well it scales to larger datasets with heterogeneous input types, by testing it out on the Diabetes dataset. Here we will also see how one can tune the decision threshold of a binary classifier with AutoGluon in order to try and achieve better Recall and performance on the positive class"
      ],
      "metadata": {
        "id": "Md3AVq_aoh_B"
      },
      "id": "Md3AVq_aoh_B"
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_train"
      ],
      "metadata": {
        "id": "PSoqCnOioqqx"
      },
      "id": "PSoqCnOioqqx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor_diabetes = TabularPredictor( # construct the predictor\n",
        "    label='binary_label', eval_metric='roc_auc'\n",
        ").fit( # call the fit method\n",
        "    diabetes_train.sample(frac=0.4), # training on a subsample of the data for faster training\n",
        "    excluded_model_types=['NN_TORCH', 'FASTAI', 'CAT'], # excluding neural nets for faster training\n",
        ")"
      ],
      "metadata": {
        "id": "V_lHDvBJo8v9"
      },
      "id": "V_lHDvBJo8v9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor_diabetes.evaluate(diabetes_test)"
      ],
      "metadata": {
        "id": "hyuN8Cl1pbaS"
      },
      "id": "hyuN8Cl1pbaS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we optimize the threshold to try and boost the f1 score\n",
        "threshold = predictor_diabetes.calibrate_decision_threshold(metric='f1')"
      ],
      "metadata": {
        "id": "qTXQon333ODy"
      },
      "id": "qTXQon333ODy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor_diabetes.set_decision_threshold(threshold)"
      ],
      "metadata": {
        "id": "VQ2tMTzZ3ZNm"
      },
      "id": "VQ2tMTzZ3ZNm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor_diabetes.evaluate(diabetes_test)"
      ],
      "metadata": {
        "id": "C3B1v1HJ3e0q"
      },
      "id": "C3B1v1HJ3e0q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_preds = predictor_diabetes.predict(diabetes_test)"
      ],
      "metadata": {
        "id": "JMsRNOxA0_hK"
      },
      "id": "JMsRNOxA0_hK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "kDdTjYFE0ptL"
      },
      "id": "kDdTjYFE0ptL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay.from_predictions(diabetes_test['binary_label'], diabetes_preds)"
      ],
      "metadata": {
        "id": "KFM1tmqY02nm"
      },
      "id": "KFM1tmqY02nm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}