{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e7feb5a2",
      "metadata": {
        "id": "e7feb5a2"
      },
      "source": [
        "# WS 12 AutoML with AutoGluon Hands on Module"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gKRURZHj11-T",
      "metadata": {
        "id": "gKRURZHj11-T"
      },
      "source": [
        "\n",
        "## 1. Introduction\n",
        "In this hands on module, we will see how to simplify the process of training high-quality, optimized machine learning models on sample datasets from UCI Machine Learning repository using the [AutoGluon](https://auto.gluon.ai/stable/index.html) package."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NiMElLZy24Zn",
      "metadata": {
        "id": "NiMElLZy24Zn"
      },
      "source": [
        "## 2. Dataset introduction and loading data from UCI ML Repository\n",
        "Now we import pacakges and load in the following three healthcare related datasets from [UCI Machine Learning Repository](https://archive.ics.uci.edu/)\n",
        "\n",
        "\n",
        "*   [Breast Cancer data](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic) from University of Wisconsin\n",
        "*   [Diabetes data](https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) representing ten years of clinical care at 130 US hospitals\n",
        "*  [A Drug reviews](https://archive.ics.uci.edu/dataset/461/drug+review+dataset+druglib+com) dataset providing patient reviews on specific drugs\n",
        "\n",
        "The first two datsets will be used to demonstrate AutoGluon's `TabularPredictor` class and how it enables us to train high-fideltiy ensemble models on data without needing to worry about pre-processing.\n",
        "\n",
        "The third dataset will allow us to explore AutoGluon's `MultiModalPredictor` and how it allows us to train models on plain-text inputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2b62dd5b",
      "metadata": {
        "id": "2b62dd5b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ucimlrepo import fetch_ucirepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "YreNspRjV_o1",
      "metadata": {
        "id": "YreNspRjV_o1"
      },
      "outputs": [],
      "source": [
        "# here we fix a random seed for reproducibility purposes\n",
        "np.random.seed(913)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oa0rkUeg5JDh",
      "metadata": {
        "id": "oa0rkUeg5JDh"
      },
      "source": [
        "Now we load in the breast cancer dataset. This dataset contains features that describe the characteristics of cell nuclei present in a digitized image taken from the fine needle aspirate of a breast mass. The labels in the data are binary/two-class, with 'B' representing a benign mass and 'M' representing a malignant mass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "eNnlKxMf43u9",
      "metadata": {
        "id": "eNnlKxMf43u9"
      },
      "outputs": [],
      "source": [
        "# now we load in the breast cancer dataset from UCI\n",
        "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = breast_cancer_wisconsin_diagnostic.data.features\n",
        "y = breast_cancer_wisconsin_diagnostic.data.targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "PT0a0HTa6U-B",
      "metadata": {
        "id": "PT0a0HTa6U-B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                  name     role         type demographic description units  \\\n",
            "0                   ID       ID  Categorical        None        None  None   \n",
            "1            Diagnosis   Target  Categorical        None        None  None   \n",
            "2              radius1  Feature   Continuous        None        None  None   \n",
            "3             texture1  Feature   Continuous        None        None  None   \n",
            "4           perimeter1  Feature   Continuous        None        None  None   \n",
            "5                area1  Feature   Continuous        None        None  None   \n",
            "6          smoothness1  Feature   Continuous        None        None  None   \n",
            "7         compactness1  Feature   Continuous        None        None  None   \n",
            "8           concavity1  Feature   Continuous        None        None  None   \n",
            "9      concave_points1  Feature   Continuous        None        None  None   \n",
            "10           symmetry1  Feature   Continuous        None        None  None   \n",
            "11  fractal_dimension1  Feature   Continuous        None        None  None   \n",
            "12             radius2  Feature   Continuous        None        None  None   \n",
            "13            texture2  Feature   Continuous        None        None  None   \n",
            "14          perimeter2  Feature   Continuous        None        None  None   \n",
            "15               area2  Feature   Continuous        None        None  None   \n",
            "16         smoothness2  Feature   Continuous        None        None  None   \n",
            "17        compactness2  Feature   Continuous        None        None  None   \n",
            "18          concavity2  Feature   Continuous        None        None  None   \n",
            "19     concave_points2  Feature   Continuous        None        None  None   \n",
            "20           symmetry2  Feature   Continuous        None        None  None   \n",
            "21  fractal_dimension2  Feature   Continuous        None        None  None   \n",
            "22             radius3  Feature   Continuous        None        None  None   \n",
            "23            texture3  Feature   Continuous        None        None  None   \n",
            "24          perimeter3  Feature   Continuous        None        None  None   \n",
            "25               area3  Feature   Continuous        None        None  None   \n",
            "26         smoothness3  Feature   Continuous        None        None  None   \n",
            "27        compactness3  Feature   Continuous        None        None  None   \n",
            "28          concavity3  Feature   Continuous        None        None  None   \n",
            "29     concave_points3  Feature   Continuous        None        None  None   \n",
            "30           symmetry3  Feature   Continuous        None        None  None   \n",
            "31  fractal_dimension3  Feature   Continuous        None        None  None   \n",
            "\n",
            "   missing_values  \n",
            "0              no  \n",
            "1              no  \n",
            "2              no  \n",
            "3              no  \n",
            "4              no  \n",
            "5              no  \n",
            "6              no  \n",
            "7              no  \n",
            "8              no  \n",
            "9              no  \n",
            "10             no  \n",
            "11             no  \n",
            "12             no  \n",
            "13             no  \n",
            "14             no  \n",
            "15             no  \n",
            "16             no  \n",
            "17             no  \n",
            "18             no  \n",
            "19             no  \n",
            "20             no  \n",
            "21             no  \n",
            "22             no  \n",
            "23             no  \n",
            "24             no  \n",
            "25             no  \n",
            "26             no  \n",
            "27             no  \n",
            "28             no  \n",
            "29             no  \n",
            "30             no  \n",
            "31             no  \n"
          ]
        }
      ],
      "source": [
        "print(breast_cancer_wisconsin_diagnostic.variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "vuhB5yTh91__",
      "metadata": {
        "id": "vuhB5yTh91__"
      },
      "outputs": [],
      "source": [
        "breast_cancer_df = X.assign(\n",
        "    Diagnosis=y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "I9tbb7hb99Ni",
      "metadata": {
        "id": "I9tbb7hb99Ni"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>radius1</th>\n",
              "      <th>texture1</th>\n",
              "      <th>perimeter1</th>\n",
              "      <th>area1</th>\n",
              "      <th>smoothness1</th>\n",
              "      <th>compactness1</th>\n",
              "      <th>concavity1</th>\n",
              "      <th>concave_points1</th>\n",
              "      <th>symmetry1</th>\n",
              "      <th>fractal_dimension1</th>\n",
              "      <th>...</th>\n",
              "      <th>texture3</th>\n",
              "      <th>perimeter3</th>\n",
              "      <th>area3</th>\n",
              "      <th>smoothness3</th>\n",
              "      <th>compactness3</th>\n",
              "      <th>concavity3</th>\n",
              "      <th>concave_points3</th>\n",
              "      <th>symmetry3</th>\n",
              "      <th>fractal_dimension3</th>\n",
              "      <th>Diagnosis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>...</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>...</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>...</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>...</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>...</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>...</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>...</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>...</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>...</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>...</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     radius1  texture1  perimeter1   area1  smoothness1  compactness1  \\\n",
              "0      17.99     10.38      122.80  1001.0      0.11840       0.27760   \n",
              "1      20.57     17.77      132.90  1326.0      0.08474       0.07864   \n",
              "2      19.69     21.25      130.00  1203.0      0.10960       0.15990   \n",
              "3      11.42     20.38       77.58   386.1      0.14250       0.28390   \n",
              "4      20.29     14.34      135.10  1297.0      0.10030       0.13280   \n",
              "..       ...       ...         ...     ...          ...           ...   \n",
              "564    21.56     22.39      142.00  1479.0      0.11100       0.11590   \n",
              "565    20.13     28.25      131.20  1261.0      0.09780       0.10340   \n",
              "566    16.60     28.08      108.30   858.1      0.08455       0.10230   \n",
              "567    20.60     29.33      140.10  1265.0      0.11780       0.27700   \n",
              "568     7.76     24.54       47.92   181.0      0.05263       0.04362   \n",
              "\n",
              "     concavity1  concave_points1  symmetry1  fractal_dimension1  ...  \\\n",
              "0       0.30010          0.14710     0.2419             0.07871  ...   \n",
              "1       0.08690          0.07017     0.1812             0.05667  ...   \n",
              "2       0.19740          0.12790     0.2069             0.05999  ...   \n",
              "3       0.24140          0.10520     0.2597             0.09744  ...   \n",
              "4       0.19800          0.10430     0.1809             0.05883  ...   \n",
              "..          ...              ...        ...                 ...  ...   \n",
              "564     0.24390          0.13890     0.1726             0.05623  ...   \n",
              "565     0.14400          0.09791     0.1752             0.05533  ...   \n",
              "566     0.09251          0.05302     0.1590             0.05648  ...   \n",
              "567     0.35140          0.15200     0.2397             0.07016  ...   \n",
              "568     0.00000          0.00000     0.1587             0.05884  ...   \n",
              "\n",
              "     texture3  perimeter3   area3  smoothness3  compactness3  concavity3  \\\n",
              "0       17.33      184.60  2019.0      0.16220       0.66560      0.7119   \n",
              "1       23.41      158.80  1956.0      0.12380       0.18660      0.2416   \n",
              "2       25.53      152.50  1709.0      0.14440       0.42450      0.4504   \n",
              "3       26.50       98.87   567.7      0.20980       0.86630      0.6869   \n",
              "4       16.67      152.20  1575.0      0.13740       0.20500      0.4000   \n",
              "..        ...         ...     ...          ...           ...         ...   \n",
              "564     26.40      166.10  2027.0      0.14100       0.21130      0.4107   \n",
              "565     38.25      155.00  1731.0      0.11660       0.19220      0.3215   \n",
              "566     34.12      126.70  1124.0      0.11390       0.30940      0.3403   \n",
              "567     39.42      184.60  1821.0      0.16500       0.86810      0.9387   \n",
              "568     30.37       59.16   268.6      0.08996       0.06444      0.0000   \n",
              "\n",
              "     concave_points3  symmetry3  fractal_dimension3  Diagnosis  \n",
              "0             0.2654     0.4601             0.11890          M  \n",
              "1             0.1860     0.2750             0.08902          M  \n",
              "2             0.2430     0.3613             0.08758          M  \n",
              "3             0.2575     0.6638             0.17300          M  \n",
              "4             0.1625     0.2364             0.07678          M  \n",
              "..               ...        ...                 ...        ...  \n",
              "564           0.2216     0.2060             0.07115          M  \n",
              "565           0.1628     0.2572             0.06637          M  \n",
              "566           0.1418     0.2218             0.07820          M  \n",
              "567           0.2650     0.4087             0.12400          M  \n",
              "568           0.0000     0.2871             0.07039          B  \n",
              "\n",
              "[569 rows x 31 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "breast_cancer_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5eKQ2XK0-MWe",
      "metadata": {
        "id": "5eKQ2XK0-MWe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Diagnosis\n",
              "B    357\n",
              "M    212\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "breast_cancer_df['Diagnosis'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VTKvVtP3QstW",
      "metadata": {
        "id": "VTKvVtP3QstW"
      },
      "source": [
        "Next we load in the Diabetes dataset. This dataset was constructed with the goal of predicting the early readmission of diabetes patients within 30 days of discharge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c3nILN1VRAYE",
      "metadata": {
        "id": "c3nILN1VRAYE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/troy/arch_workshop_automl_ws14/venv/lib/python3.10/site-packages/ucimlrepo/fetch.py:97: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(data_url)\n"
          ]
        }
      ],
      "source": [
        "# fetch dataset\n",
        "diabetes_data = fetch_ucirepo(id=296)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = diabetes_data.data.features\n",
        "y = diabetes_data.data.targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "x4y6ROy8RG9q",
      "metadata": {
        "id": "x4y6ROy8RG9q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                        name     role         type demographic  \\\n",
            "0               encounter_id       ID                     None   \n",
            "1                patient_nbr       ID                     None   \n",
            "2                       race  Feature  Categorical        Race   \n",
            "3                     gender  Feature  Categorical      Gender   \n",
            "4                        age  Feature  Categorical         Age   \n",
            "5                     weight  Feature  Categorical        None   \n",
            "6          admission_type_id  Feature  Categorical        None   \n",
            "7   discharge_disposition_id  Feature  Categorical        None   \n",
            "8        admission_source_id  Feature  Categorical        None   \n",
            "9           time_in_hospital  Feature      Integer        None   \n",
            "10                payer_code  Feature  Categorical        None   \n",
            "11         medical_specialty  Feature  Categorical        None   \n",
            "12        num_lab_procedures  Feature      Integer        None   \n",
            "13            num_procedures  Feature      Integer        None   \n",
            "14           num_medications  Feature      Integer        None   \n",
            "15         number_outpatient  Feature      Integer        None   \n",
            "16          number_emergency  Feature      Integer        None   \n",
            "17          number_inpatient  Feature      Integer        None   \n",
            "18                    diag_1  Feature  Categorical        None   \n",
            "19                    diag_2  Feature  Categorical        None   \n",
            "20                    diag_3  Feature  Categorical        None   \n",
            "21          number_diagnoses  Feature      Integer        None   \n",
            "22             max_glu_serum  Feature  Categorical        None   \n",
            "23                 A1Cresult  Feature  Categorical        None   \n",
            "24                 metformin  Feature  Categorical        None   \n",
            "25               repaglinide  Feature  Categorical        None   \n",
            "26               nateglinide  Feature  Categorical        None   \n",
            "27            chlorpropamide  Feature  Categorical        None   \n",
            "28               glimepiride  Feature  Categorical        None   \n",
            "29             acetohexamide  Feature  Categorical        None   \n",
            "30                 glipizide  Feature  Categorical        None   \n",
            "31                 glyburide  Feature  Categorical        None   \n",
            "32               tolbutamide  Feature  Categorical        None   \n",
            "33              pioglitazone  Feature  Categorical        None   \n",
            "34             rosiglitazone  Feature  Categorical        None   \n",
            "35                  acarbose  Feature  Categorical        None   \n",
            "36                  miglitol  Feature  Categorical        None   \n",
            "37              troglitazone  Feature  Categorical        None   \n",
            "38                tolazamide  Feature  Categorical        None   \n",
            "39                   examide  Feature  Categorical        None   \n",
            "40               citoglipton  Feature  Categorical        None   \n",
            "41                   insulin  Feature  Categorical        None   \n",
            "42       glyburide-metformin  Feature  Categorical        None   \n",
            "43       glipizide-metformin  Feature  Categorical        None   \n",
            "44  glimepiride-pioglitazone  Feature  Categorical        None   \n",
            "45   metformin-rosiglitazone  Feature  Categorical        None   \n",
            "46    metformin-pioglitazone  Feature  Categorical        None   \n",
            "47                    change  Feature  Categorical        None   \n",
            "48               diabetesMed  Feature  Categorical        None   \n",
            "49                readmitted   Target  Categorical        None   \n",
            "\n",
            "                                          description units missing_values  \n",
            "0                   Unique identifier of an encounter  None             no  \n",
            "1                      Unique identifier of a patient  None             no  \n",
            "2   Values: Caucasian, Asian, African American, Hi...  None            yes  \n",
            "3           Values: male, female, and unknown/invalid  None             no  \n",
            "4   Grouped in 10-year intervals: [0, 10), [10, 20...  None             no  \n",
            "5                                   Weight in pounds.  None            yes  \n",
            "6   Integer identifier corresponding to 9 distinct...  None             no  \n",
            "7   Integer identifier corresponding to 29 distinc...  None             no  \n",
            "8   Integer identifier corresponding to 21 distinc...  None             no  \n",
            "9   Integer number of days between admission and d...  None             no  \n",
            "10  Integer identifier corresponding to 23 distinc...  None            yes  \n",
            "11  Integer identifier of a specialty of the admit...  None            yes  \n",
            "12  Number of lab tests performed during the encou...  None             no  \n",
            "13  Number of procedures (other than lab tests) pe...  None             no  \n",
            "14  Number of distinct generic names administered ...  None             no  \n",
            "15  Number of outpatient visits of the patient in ...  None             no  \n",
            "16  Number of emergency visits of the patient in t...  None             no  \n",
            "17  Number of inpatient visits of the patient in t...  None             no  \n",
            "18  The primary diagnosis (coded as first three di...  None            yes  \n",
            "19  Secondary diagnosis (coded as first three digi...  None            yes  \n",
            "20  Additional secondary diagnosis (coded as first...  None            yes  \n",
            "21          Number of diagnoses entered to the system  None             no  \n",
            "22  Indicates the range of the result or if the te...  None             no  \n",
            "23  Indicates the range of the result or if the te...  None             no  \n",
            "24  The feature indicates whether the drug was pre...  None             no  \n",
            "25  The feature indicates whether the drug was pre...  None             no  \n",
            "26  The feature indicates whether the drug was pre...  None             no  \n",
            "27  The feature indicates whether the drug was pre...  None             no  \n",
            "28  The feature indicates whether the drug was pre...  None             no  \n",
            "29  The feature indicates whether the drug was pre...  None             no  \n",
            "30  The feature indicates whether the drug was pre...  None             no  \n",
            "31  The feature indicates whether the drug was pre...  None             no  \n",
            "32  The feature indicates whether the drug was pre...  None             no  \n",
            "33  The feature indicates whether the drug was pre...  None             no  \n",
            "34  The feature indicates whether the drug was pre...  None             no  \n",
            "35  The feature indicates whether the drug was pre...  None             no  \n",
            "36  The feature indicates whether the drug was pre...  None             no  \n",
            "37  The feature indicates whether the drug was pre...  None             no  \n",
            "38  The feature indicates whether the drug was pre...  None             no  \n",
            "39  The feature indicates whether the drug was pre...  None             no  \n",
            "40  The feature indicates whether the drug was pre...  None             no  \n",
            "41  The feature indicates whether the drug was pre...  None             no  \n",
            "42  The feature indicates whether the drug was pre...  None             no  \n",
            "43  The feature indicates whether the drug was pre...  None             no  \n",
            "44  The feature indicates whether the drug was pre...  None             no  \n",
            "45  The feature indicates whether the drug was pre...  None             no  \n",
            "46  The feature indicates whether the drug was pre...  None             no  \n",
            "47  Indicates if there was a change in diabetic me...  None             no  \n",
            "48  Indicates if there was any diabetic medication...  None             no  \n",
            "49  Days to inpatient readmission. Values: <30 if ...  None             no  \n"
          ]
        }
      ],
      "source": [
        "print(diabetes_data.variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "abIT4UzIEG92",
      "metadata": {
        "id": "abIT4UzIEG92"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>readmitted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&gt;30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101761</th>\n",
              "      <td>&gt;30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101762</th>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101763</th>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101764</th>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101765</th>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>101766 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       readmitted\n",
              "0              NO\n",
              "1             >30\n",
              "2              NO\n",
              "3              NO\n",
              "4              NO\n",
              "...           ...\n",
              "101761        >30\n",
              "101762         NO\n",
              "101763         NO\n",
              "101764         NO\n",
              "101765         NO\n",
              "\n",
              "[101766 rows x 1 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "JKzx07jxRMhH",
      "metadata": {
        "id": "JKzx07jxRMhH"
      },
      "outputs": [],
      "source": [
        "diabetes_df = X.assign(\n",
        "    readmitted=y.map(lambda readmit: 0 if readmit == 'NO' else 1) # convert to a binary target\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "Oe1LpCMARRBq",
      "metadata": {
        "id": "Oe1LpCMARRBq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "readmitted\n",
              "0    54864\n",
              "1    46902\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "diabetes_df['readmitted'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b1JRJNviEOcg",
      "metadata": {
        "id": "b1JRJNviEOcg"
      },
      "outputs": [],
      "source": [
        "# because this is such a large dataset, we will down-sample this to only include 20% of the dat\n",
        "diabetes_df_downsamp = diabetes_df.sample(frac=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "HrIE7YB7EV_s",
      "metadata": {
        "id": "HrIE7YB7EV_s"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "readmitted\n",
              "0    10912\n",
              "1     9441\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "diabetes_df_downsamp['readmitted'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PciRQPzH639S",
      "metadata": {
        "id": "PciRQPzH639S"
      },
      "source": [
        "Now we split the two datasets into 80%/20% training/test set splits, so that we can evaluate our tuned models at the very end on unseen test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "MDdZzW2OSvmO",
      "metadata": {
        "id": "MDdZzW2OSvmO"
      },
      "outputs": [],
      "source": [
        "bc_train = breast_cancer_df.sample(frac=0.8)\n",
        "bc_test = breast_cancer_df.drop(bc_train.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "N7UATHpSS3Og",
      "metadata": {
        "id": "N7UATHpSS3Og"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Diagnosis\n",
              "B    0.608791\n",
              "M    0.391209\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bc_train['Diagnosis'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "wA7YrbzcS5-v",
      "metadata": {
        "id": "wA7YrbzcS5-v"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Diagnosis\n",
              "B    0.701754\n",
              "M    0.298246\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bc_test['Diagnosis'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "UhxSy-bTTGJi",
      "metadata": {
        "id": "UhxSy-bTTGJi"
      },
      "outputs": [],
      "source": [
        "diabetes_train = diabetes_df_downsamp.sample(frac=0.8)\n",
        "diabetes_test = diabetes_df_downsamp.drop(diabetes_train.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "FNIVuqVDTOBR",
      "metadata": {
        "id": "FNIVuqVDTOBR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "readmitted\n",
              "0    0.535684\n",
              "1    0.464316\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "diabetes_train['readmitted'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "sCAS2hIwTQcf",
      "metadata": {
        "id": "sCAS2hIwTQcf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "readmitted\n",
              "0    0.537951\n",
              "1    0.462049\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "diabetes_test['readmitted'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09Bv3a8n8ZsE",
      "metadata": {
        "id": "09Bv3a8n8ZsE"
      },
      "source": [
        "## 3. Introduction to AutoGluon Tabular Predictor\n",
        "Now we will see how AutoGluon's `TabularPredictor` class can be used to automatically fit a weighted ensemble on the breast cancer dataset, with automatic K-fold cross validation, bagging, and stacking, and with a large suite of models evaluated for inclusion in the final ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "SlqyUWsF8nje",
      "metadata": {
        "id": "SlqyUWsF8nje"
      },
      "outputs": [],
      "source": [
        "from autogluon.tabular import TabularPredictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "M2uLil1SFC9d",
      "metadata": {
        "id": "M2uLil1SFC9d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20260219_192117\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.5.0\n",
            "Python Version:     3.10.16\n",
            "Operating System:   Darwin\n",
            "Platform Machine:   arm64\n",
            "Platform Version:   Darwin Kernel Version 24.6.0: Wed Nov  5 21:32:34 PST 2025; root:xnu-11417.140.69.705.2~1/RELEASE_ARM64_T6020\n",
            "CPU Count:          12\n",
            "Pytorch Version:    2.9.1\n",
            "CUDA Version:       CUDA is not available\n",
            "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
            "Memory Avail:       55.74 GB / 96.00 GB (58.1%)\n",
            "Disk Space Avail:   509.52 GB / 926.35 GB (55.0%)\n",
            "===================================================\n",
            "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
            "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
            "\tpresets='extreme'  : New in v1.5: The state-of-the-art for tabular data. Massively better than 'best' on datasets <100000 samples by using new Tabular Foundation Models (TFMs) meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, TabDPT, and TabM. Requires a GPU and `pip install autogluon.tabular[tabarena]` to install TabPFN, TabICL, and TabDPT.\n",
            "\tpresets='best'     : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
            "\tpresets='best_v150': New in v1.5: Better quality than 'best' and 5x+ faster to train. Give it a try!\n",
            "\tpresets='high'     : Strong accuracy with fast inference speed.\n",
            "\tpresets='high_v150': New in v1.5: Better quality than 'high' and 5x+ faster to train. Give it a try!\n",
            "\tpresets='good'     : Good accuracy with very fast inference speed.\n",
            "\tpresets='medium'   : Fast training time, ideal for initial prototyping.\n",
            "Using hyperparameters preset: hyperparameters='default'\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"/Users/troy/arch_workshop_automl_ws14/AutogluonModels/ag-20260219_192117\"\n",
            "Train Data Rows:    455\n",
            "Train Data Columns: 30\n",
            "Label Column:       Diagnosis\n",
            "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
            "\t2 unique label values:  ['B', 'M']\n",
            "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 = M, class 0 = B\n",
            "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (M) vs negative (B) class.\n",
            "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    57052.62 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.10 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 30 | ['radius1', 'texture1', 'perimeter1', 'area1', 'smoothness1', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 30 | ['radius1', 'texture1', 'perimeter1', 'area1', 'smoothness1', ...]\n",
            "\t0.0s = Fit runtime\n",
            "\t30 features in original data used to generate 30 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.10 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.03s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
            "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}],\n",
            "\t'XGB': [{}],\n",
            "\t'FASTAI': [{}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "Excluded models: ['CAT', 'FASTAI', 'NN_TORCH'] (Specified by `excluded_model_types`)\n",
            "Fitting 8 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ...\n",
            "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=4, gpus=0, memory=0.04%)\n",
            "/Users/troy/arch_workshop_automl_ws14/venv/lib/python3.10/site-packages/ray/_private/worker.py:2062: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
            "  warnings.warn(\n",
            "\t0.9937\t = Validation score   (roc_auc)\n",
            "\t2.59s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ...\n",
            "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=4, gpus=0, memory=0.04%)\n",
            "\t0.9883\t = Validation score   (roc_auc)\n",
            "\t0.75s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: RandomForestGini_BAG_L1 ...\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=12, gpus=0\n",
            "\t0.9874\t = Validation score   (roc_auc)\n",
            "\t2.08s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr_BAG_L1 ...\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=12, gpus=0\n",
            "\t0.9894\t = Validation score   (roc_auc)\n",
            "\t0.19s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini_BAG_L1 ...\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=12, gpus=0\n",
            "\t0.9915\t = Validation score   (roc_auc)\n",
            "\t0.21s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr_BAG_L1 ...\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=12, gpus=0\n",
            "\t0.9917\t = Validation score   (roc_auc)\n",
            "\t0.19s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ...\n",
            "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=4, gpus=0, memory=0.03%)\n",
            "\t0.9899\t = Validation score   (roc_auc)\n",
            "\t0.55s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ...\n",
            "\tFitting 3 child models (S1F1 - S1F3) | Fitting with ParallelLocalFoldFittingStrategy (3 workers, per: cpus=4, gpus=0, memory=0.15%)\n",
            "\t0.9805\t = Validation score   (roc_auc)\n",
            "\t2.81s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\tFitting 1 model on all data | Fitting with cpus=12, gpus=0, mem=0.0/55.0 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.929, 'XGBoost_BAG_L1': 0.071}\n",
            "\t0.9937\t = Validation score   (roc_auc)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 21.46s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 23687.9 rows/s (152 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/troy/arch_workshop_automl_ws14/AutogluonModels/ag-20260219_192117\")\n"
          ]
        }
      ],
      "source": [
        "# here we fit a tabular predictor on the breast cancer dataset\n",
        "predictor_bc = TabularPredictor( # construct the predictor\n",
        "    label='Diagnosis', eval_metric='roc_auc' # specify 'Diagnosis' as the label/target, and AUROC as the performance metric\n",
        ").fit( # call the fit method\n",
        "    bc_train,\n",
        "    num_bag_folds=3, # specifies how many folds to run k-Fold cross validation with\n",
        "    excluded_model_types=['NN_TORCH', 'FASTAI', 'CAT'] # exclude neural nets and CatBoost for faster training\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K3ElcsjwGjfD",
      "metadata": {
        "id": "K3ElcsjwGjfD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'roc_auc': np.float64(1.0),\n",
              " 'accuracy': 0.9912280701754386,\n",
              " 'balanced_accuracy': np.float64(0.99375),\n",
              " 'mcc': 0.9794313218831192,\n",
              " 'f1': 0.9855072463768116,\n",
              " 'precision': 0.9714285714285714,\n",
              " 'recall': 1.0}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "*** SIGTERM received at time=1771528943 ***\n",
            "PC: @        0x19e037d04  (unknown)  kevent\n",
            "    @        0x318d07a2c  (unknown)  absl::lts_20230802::AbslFailureSignalHandler()\n",
            "    @        0x19e0ad6a4  (unknown)  _sigtramp\n",
            "    @        0x10516cb68  (unknown)  select_kqueue_control_impl\n",
            "    @        0x104a7259c  (unknown)  method_vectorcall_FASTCALL\n",
            "    @        0x104b78e90  (unknown)  _PyEval_EvalFrameDefault\n",
            "    @        0x104a63320  (unknown)  _PyFunction_Vectorcall\n",
            "    @        0x104b78e90  (unknown)  _PyEval_EvalFrameDefault\n",
            "    @        0x104a63320  (unknown)  _PyFunction_Vectorcall\n",
            "    @        0x104b78e90  (unknown)  _PyEval_EvalFrameDefault\n",
            "    @        0x104a63320  (unknown)  _PyFunction_Vectorcall\n",
            "    @        0x104b78e90  (unknown)  _PyEval_EvalFrameDefault\n",
            "    @        0x104a63320  (unknown)  _PyFunction_Vectorcall\n",
            "    @        0x104b78e90  (unknown)  _PyEval_EvalFrameDefault\n",
            "    @        0x104a63320  (unknown)  _PyFunction_Vectorcall\n",
            "    @        0x104b78e90  (unknown)  _PyEval_EvalFrameDefault\n",
            "    @        0x104a63320  (unknown)  _PyFunction_Vectorcall\n",
            "    @        0x104a679e0  (unknown)  method_vectorcall\n",
            "    @        0x104b7a670  (unknown)  _PyEval_EvalFrameDefault\n",
            "    @        0x104b6ec5c  (unknown)  _PyEval_Vector\n",
            "    @        0x104b696a4  (unknown)  builtin_exec\n",
            "    @        0x104ac0f08  (unknown)  cfunction_vectorcall_FASTCALL\n",
            "    @        0x104b997c8  (unknown)  call_function\n",
            "    @        0x104b728c8  (unknown)  _PyEval_EvalFrameDefault\n",
            "    @        0x104a63320  (unknown)  _PyFunction_Vectorcall\n",
            "    @        0x104b997c8  (unknown)  call_function\n",
            "    @        0x104b728c8  (unknown)  _PyEval_EvalFrameDefault\n",
            "    @        0x104a63320  (unknown)  _PyFunction_Vectorcall\n",
            "    @        0x104c0aeec  (unknown)  pymain_run_module\n",
            "    @        0x104c0a8d4  (unknown)  pymain_run_python\n",
            "    @        0x104c0a7c0  (unknown)  Py_RunMain\n",
            "    @        0x104a062ac  (unknown)  main\n",
            "    @        0x19dcd2b98  (unknown)  start\n",
            "[2026-02-19 09:22:23,167 E 30170 19032572] logging.cc:474: *** SIGTERM received at time=1771528943 ***\n",
            "[2026-02-19 09:22:23,167 E 30170 19032572] logging.cc:474: PC: @        0x19e037d04  (unknown)  kevent\n",
            "[2026-02-19 09:22:23,167 E 30170 19032572] logging.cc:474:     @        0x318d07b4c  (unknown)  absl::lts_20230802::AbslFailureSignalHandler()\n",
            "[2026-02-19 09:22:23,167 E 30170 19032572] logging.cc:474:     @        0x19e0ad6a4  (unknown)  _sigtramp\n",
            "[2026-02-19 09:22:23,167 E 30170 19032572] logging.cc:474:     @        0x10516cb68  (unknown)  select_kqueue_control_impl\n",
            "[2026-02-19 09:22:23,167 E 30170 19032572] logging.cc:474:     @        0x104a7259c  (unknown)  method_vectorcall_FASTCALL\n",
            "[2026-02-19 09:22:23,167 E 30170 19032572] logging.cc:474:     @        0x104b78e90  (unknown)  _PyEval_EvalFrameDefault\n",
            "[2026-02-19 09:22:23,167 E 30170 19032572] logging.cc:474:     @        0x104a63320  (unknown)  _PyFunction_Vectorcall\n",
            "[2026-02-19 09:22:23,167 E 30170 19032572] logging.cc:474:     @        0x104b78e90  (unknown)  _PyEval_EvalFrameDefault\n",
            "[2026-02-19 09:22:23,167 E 30170 19032572] logging.cc:474:     @        0x104a63320  (unknown)  _PyFunction_Vectorcall\n",
            "[2026-02-19 09:22:23,167 E 30170 19032572] logging.cc:474:     @        0x104b78e90  (unknown)  _PyEval_EvalFrameDefault\n",
            "[2026-02-19 09:22:23,167 E 30170 19032572] logging.cc:474:     @        0x104a63320  (unknown)  _PyFunction_Vectorcall\n",
            "[2026-02-19 09:22:23,167 E 30170 19032572] logging.cc:474:     @        0x104b78e90  (unknown)  _PyEval_EvalFrameDefault\n",
            "[2026-02-19 09:22:23,167 E 30170 19032572] logging.cc:474:     @        0x104a63320  (unknown)  _PyFunction_Vectorcall\n",
            "[2026-02-19 09:22:23,167 E 30170 19032572] logging.cc:474:     @        0x104b78e90  (unknown)  _PyEval_EvalFrameDefault\n",
            "[2026-02-19 09:22:23,167 E 30170 19032572] logging.cc:474:     @        0x104a63320  (unknown)  _PyFunction_Vectorcall\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x104b78e90  (unknown)  _PyEval_EvalFrameDefault\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x104a63320  (unknown)  _PyFunction_Vectorcall\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x104a679e0  (unknown)  method_vectorcall\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x104b7a670  (unknown)  _PyEval_EvalFrameDefault\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x104b6ec5c  (unknown)  _PyEval_Vector\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x104b696a4  (unknown)  builtin_exec\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x104ac0f08  (unknown)  cfunction_vectorcall_FASTCALL\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x104b997c8  (unknown)  call_function\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x104b728c8  (unknown)  _PyEval_EvalFrameDefault\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x104a63320  (unknown)  _PyFunction_Vectorcall\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x104b997c8  (unknown)  call_function\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x104b728c8  (unknown)  _PyEval_EvalFrameDefault\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x104a63320  (unknown)  _PyFunction_Vectorcall\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x104c0aeec  (unknown)  pymain_run_module\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x104c0a8d4  (unknown)  pymain_run_python\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x104c0a7c0  (unknown)  Py_RunMain\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x104a062ac  (unknown)  main\n",
            "[2026-02-19 09:22:23,168 E 30170 19032572] logging.cc:474:     @        0x19dcd2b98  (unknown)  start\n"
          ]
        }
      ],
      "source": [
        "# Now we evaluate the Breast Cancer model on the test data\n",
        "predictor_bc.evaluate(bc_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jD-1EZFFGwsv",
      "metadata": {
        "id": "jD-1EZFFGwsv"
      },
      "source": [
        "## 4. Comparison with Scikit Learn Toy Implementation of Bagging + Stacking\n",
        "We see that our accuracy and precision rival that of the benchmark models listed on the UCI Machine Learning repository page for this dataset.\n",
        "\n",
        "\n",
        "Now, for illustrative purposes, we will take a brief look at how much code it would take to implement a similar *(highly simplified)* k-fold bagging + stacking model ensembling such as what AutoGluon does automatically using Scikit Learn, another popular machine learning framework for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7rHfS2Z7V9Tm",
      "metadata": {
        "id": "7rHfS2Z7V9Tm"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from xgboost import XGBClassifier\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m1ENeFkuWe0k",
      "metadata": {
        "id": "m1ENeFkuWe0k"
      },
      "outputs": [],
      "source": [
        "# our target labels are text character 'M' and 'B'\n",
        "# Scikit-learn binary classifiers need these to be converted to numeric 1/0\n",
        "bc_train_binary = bc_train.assign(\n",
        "    binary_label=lambda x: x['Diagnosis'].map(lambda diag: 1 if diag == 'M' else 0)\n",
        ").drop(columns='Diagnosis')\n",
        "bc_test_binary = bc_test.assign(\n",
        "    binary_label=lambda x: x['Diagnosis'].map(lambda diag: 1 if diag == 'M' else 0)\n",
        ").drop(columns='Diagnosis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tgH2TZ5PWHfD",
      "metadata": {
        "id": "tgH2TZ5PWHfD"
      },
      "outputs": [],
      "source": [
        "# separate features X from targets y\n",
        "X = bc_train_binary.drop(columns=['binary_label'])\n",
        "y = bc_train_binary['binary_label']\n",
        "# initialize the Kfold object for doing kfold cross validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# construct arrays for storing the out of fold prediciotns for the models\n",
        "oof_preds_rf = np.zeros(len(X))\n",
        "oof_preds_xgb = np.zeros(len(X))\n",
        "\n",
        "# save the bagged models in lists\n",
        "\n",
        "# specifiy the classifiers that will be in each layer\n",
        "layers = [RandomForestClassifier, XGBClassifier]\n",
        "layer_preds = [oof_preds_rf, oof_preds_xgb]\n",
        "layer_bags = [list(), list()]\n",
        "# loop over our layers\n",
        "for i, layer in enumerate(layers):\n",
        "  print(f\"Performing k-fold cross validation at layer {i} with {layer}\")\n",
        "  # do the K-fold cross validation loop\n",
        "  for train_idx, val_idx in tqdm(kf.split(X), total=5):\n",
        "      # split inputs and outputs into training and validation\n",
        "      X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "      y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "      # if we are in a layer past the first layer, the inputs need to include\n",
        "      # the predictions from the prior layer\n",
        "      if i > 0:\n",
        "        X_train = np.column_stack([\n",
        "            X_train.to_numpy(),\n",
        "            layer_preds[i-1][train_idx] # include preds from prior layer\n",
        "        ])\n",
        "        X_val = np.column_stack([\n",
        "            X_val.to_numpy(),\n",
        "            layer_preds[i-1][val_idx]\n",
        "        ])\n",
        "\n",
        "      # fit a model from the given layer on the training fold\n",
        "      model = layer()\n",
        "      model.fit(X_train, y_train)\n",
        "      # evaluate it on the validation fold and save oof predictions\n",
        "      layer_preds[i][val_idx] = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "      # save the model in our layer bag\n",
        "      layer_bags[i].append(model)\n",
        "\n",
        "# final meta model: Weighted ensemble of the predictions from the prior layers\n",
        "meta_features = np.column_stack(layer_preds)\n",
        "\n",
        "meta_model = LogisticRegression()\n",
        "meta_model.fit(meta_features, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74LPiyADbaz7",
      "metadata": {
        "id": "74LPiyADbaz7"
      },
      "source": [
        "Now to evaluate this custom Meta model on the unseen test data, we need to\n",
        "\n",
        "\n",
        "1.   Get 5 separate sets of predictions from each Random Forest model in the first layer\n",
        "2.   Get 5 separate sets of predictions from each XGBoost model in the second layer, appending the predictions from the first layer models as feature inputs to the second layer models\n",
        "3. Average the predicions made at each layer into a single set of predictions per layer\n",
        "4. Append these two sets of predictions together into the final features to feed to the Meta Model (Logistic Regression)\n",
        "5. Get the final predictions from the Meta Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m5xTfZBVbBE8",
      "metadata": {
        "id": "m5xTfZBVbBE8"
      },
      "outputs": [],
      "source": [
        "X_test = bc_test_binary.drop(columns=['binary_label'])\n",
        "y_test = bc_test_binary['binary_label']\n",
        "\n",
        "layer_preds_test = []\n",
        "\n",
        "# steps 1 and 2 - Getting separate predictions from models in each layer\n",
        "for i in range(len(layer_bags)):\n",
        "  # concatenate predictions from prior layer to features if needed\n",
        "  if i > 0:\n",
        "    X_test_stacked = np.column_stack([X_test.to_numpy(), layer_preds_test[i-1]])\n",
        "  else:\n",
        "    X_test_stacked = X_test\n",
        "\n",
        "  # step 3 - compute average predictions across all models in the layer\n",
        "  current_layer_preds = sum([\n",
        "      model.predict_proba(X_test_stacked)[:, 1]\n",
        "      for model in layer_bags[i]\n",
        "  ]) / len(layer_bags[i])\n",
        "\n",
        "  # save the predictions for the layer\n",
        "  layer_preds_test.append(current_layer_preds)\n",
        "\n",
        "# Step 4 - concatenate layer predicitons\n",
        "meta_features_test = np.column_stack(layer_preds_test)\n",
        "\n",
        "# Step 5 - get final predictions\n",
        "final_predictions = meta_model.predict(meta_features_test)\n",
        "\n",
        "# get the accuracy and f1 score\n",
        "print(f\"F1: {f1_score(y_test, final_predictions)}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, final_predictions)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i_3rXJganwrt",
      "metadata": {
        "id": "i_3rXJganwrt"
      },
      "source": [
        "## 5. Applying AutoGluon to a more complex dataset and threshold tuning\n",
        "We can see that a considerable amount of code, understanding, and index manipulation was required in order to get an ensemble implementation using Scikit-learn that approaches the functionality which AutoGluon provides in only 3-4 lines of code. The toy implementation above also does not perform any greedy model weighting or pruning of models, and only incorporated two model families in the ensemble.\n",
        "\n",
        "Here we train another TabularPredictor on the Diabetes readmission data, to assess how well it scales to larger datasets with heterogeneous input types. Here we will also see how one can tune the decision threshold of a binary classifier with AutoGluon in order to try and achieve better Recall and performance on the positive class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PSoqCnOioqqx",
      "metadata": {
        "id": "PSoqCnOioqqx"
      },
      "outputs": [],
      "source": [
        "diabetes_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V_lHDvBJo8v9",
      "metadata": {
        "id": "V_lHDvBJo8v9"
      },
      "outputs": [],
      "source": [
        "predictor_diabetes = TabularPredictor( # construct the predictor\n",
        "    label='readmitted', eval_metric='roc_auc'\n",
        ").fit( # call the fit method\n",
        "    train_data=diabetes_train,\n",
        "    excluded_model_types=['NN_TORCH', 'FASTAI', 'CAT'], # excluding neural nets for faster training\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hyuN8Cl1pbaS",
      "metadata": {
        "id": "hyuN8Cl1pbaS"
      },
      "outputs": [],
      "source": [
        "predictor_diabetes.evaluate(diabetes_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NzoSVhKyKPPd",
      "metadata": {
        "id": "NzoSVhKyKPPd"
      },
      "source": [
        "The recall of 0.54 tells us that only ~54% of patients who were readmitted to a hospital were successfully classified as such by our model. If we want to tune our classification threshold to achieve a better recall/sensitivity/TPR (true-positive rate), as we often do when working with medical data and developing tests for the presence of risks/conditions, AutoGluon makes this very easy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kDdTjYFE0ptL",
      "metadata": {
        "id": "kDdTjYFE0ptL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PXnNRZAEK3Mu",
      "metadata": {
        "id": "PXnNRZAEK3Mu"
      },
      "outputs": [],
      "source": [
        "diabetes_preds = predictor_diabetes.predict(diabetes_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_tVwXKJvK5fR",
      "metadata": {
        "id": "_tVwXKJvK5fR"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(diabetes_test['readmitted'], diabetes_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qTXQon333ODy",
      "metadata": {
        "id": "qTXQon333ODy"
      },
      "outputs": [],
      "source": [
        "# now we calibrate the decision threshold of our model, using the F1 score as the calibration metric\n",
        "threshold = predictor_diabetes.calibrate_decision_threshold(metric='f1')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VcgRghAoJ4jJ",
      "metadata": {
        "id": "VcgRghAoJ4jJ"
      },
      "source": [
        "Conceptual Question - Why did we calibrate the decision threshold to optimize the [F1 score](https://en.wikipedia.org/wiki/F-score) (the harmonic mean of precision and recall) if our goal was to achieve higher recall? Why not just use recall as the calibration metric directly?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VQ2tMTzZ3ZNm",
      "metadata": {
        "id": "VQ2tMTzZ3ZNm"
      },
      "outputs": [],
      "source": [
        "predictor_diabetes.set_decision_threshold(threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C3B1v1HJ3e0q",
      "metadata": {
        "id": "C3B1v1HJ3e0q"
      },
      "outputs": [],
      "source": [
        "predictor_diabetes.evaluate(diabetes_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dD7xNZvbLI6A",
      "metadata": {
        "id": "dD7xNZvbLI6A"
      },
      "outputs": [],
      "source": [
        "diabetes_preds = predictor_diabetes.predict(diabetes_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D-luEmxALJl9",
      "metadata": {
        "id": "D-luEmxALJl9"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(diabetes_test['readmitted'], diabetes_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fzxUGnHpLq6v",
      "metadata": {
        "id": "fzxUGnHpLq6v"
      },
      "source": [
        "\n",
        "## 6. Tackling a Text Classification Problem with AutoGluon Tabular and MultiModal\n",
        "Now that we will see how AutoGluon can also seamlessly tackle plain-text columns, such as written reviews, using both the `TabularPredictor` class, as well as the more sophisticated `MultiModalPredictor` which trains a full transformer neural network to make predictions on text-based data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-FbgZFX4ME97",
      "metadata": {
        "id": "-FbgZFX4ME97"
      },
      "outputs": [],
      "source": [
        "from autogluon.multimodal import MultiModalPredictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MzzX1RXVMIOs",
      "metadata": {
        "id": "MzzX1RXVMIOs"
      },
      "outputs": [],
      "source": [
        "# now we load in the Drug Reviews dataset from UCI Machine learning repository\n",
        "drug_reviews_druglib_com = fetch_ucirepo(id=461)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = drug_reviews_druglib_com.data.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "agG-ZW5kMZ2G",
      "metadata": {
        "id": "agG-ZW5kMZ2G"
      },
      "outputs": [],
      "source": [
        "print(drug_reviews_druglib_com.variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0x5UliUVNnyS",
      "metadata": {
        "id": "0x5UliUVNnyS"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QhrSSYhwNV7A",
      "metadata": {
        "id": "QhrSSYhwNV7A"
      },
      "outputs": [],
      "source": [
        "# construct a 3-class label based on the numeric rating from 1-10\n",
        "targets = X['rating'].map(lambda rating: 'positive' if rating >= 7 else 'neutral' if rating >= 4 else 'negative')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QkhjhDZHMh44",
      "metadata": {
        "id": "QkhjhDZHMh44"
      },
      "outputs": [],
      "source": [
        "drug_reviews_df = X[['benefitsReview', 'sideEffectsReview', 'commentsReview']].assign(\n",
        "    target=targets\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EkDaaqnINkSH",
      "metadata": {
        "id": "EkDaaqnINkSH"
      },
      "outputs": [],
      "source": [
        "drug_reviews_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TfRMVoM5Nzv6",
      "metadata": {
        "id": "TfRMVoM5Nzv6"
      },
      "outputs": [],
      "source": [
        "drug_reviews_df['target'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bqfaVdGCRv4S",
      "metadata": {
        "id": "bqfaVdGCRv4S"
      },
      "outputs": [],
      "source": [
        "# we will downsample the three classes to achieve a smaller dataset for demonstration purposes\n",
        "drug_reviews_downsamp = pd.concat([\n",
        "    drug_reviews_df.query('target == \"positive\"').sample(n=400),\n",
        "    drug_reviews_df.query('target == \"neutral\"').sample(n=400),\n",
        "    drug_reviews_df.query('target == \"negative\"').sample(n=400)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "piD3hOA9N9LI",
      "metadata": {
        "id": "piD3hOA9N9LI"
      },
      "outputs": [],
      "source": [
        "drug_reviews_train = drug_reviews_downsamp.sample(frac=0.8)\n",
        "drug_reviews_test = drug_reviews_downsamp.drop(drug_reviews_train.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jSmiNc8ROGkt",
      "metadata": {
        "id": "jSmiNc8ROGkt"
      },
      "outputs": [],
      "source": [
        "drug_reviews_train['target'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BrV0BoeZOJ7R",
      "metadata": {
        "id": "BrV0BoeZOJ7R"
      },
      "outputs": [],
      "source": [
        "drug_reviews_test['target'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y7drEuRnkNpX",
      "metadata": {
        "id": "y7drEuRnkNpX"
      },
      "outputs": [],
      "source": [
        "# first we fit a TabularPredictor to the dataset to see how it performs\n",
        "predictor_tab = TabularPredictor(label='target', eval_metric='acc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3JU2KY68kVSr",
      "metadata": {
        "id": "3JU2KY68kVSr"
      },
      "outputs": [],
      "source": [
        "predictor_tab.fit(drug_reviews_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S98glnhPku_c",
      "metadata": {
        "id": "S98glnhPku_c"
      },
      "outputs": [],
      "source": [
        "# now we evaluate the model's performance on the test data\n",
        "predictor_tab.evaluate(drug_reviews_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tRKokVu8k_2f",
      "metadata": {
        "id": "tRKokVu8k_2f"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(drug_reviews_test['target'], predictor_tab.predict(drug_reviews_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pC6M7ckqlMSD",
      "metadata": {
        "id": "pC6M7ckqlMSD"
      },
      "outputs": [],
      "source": [
        "print(classification_report(drug_reviews_test['target'], predictor_tab.predict(drug_reviews_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pG79RvWoMjqf",
      "metadata": {
        "id": "pG79RvWoMjqf"
      },
      "outputs": [],
      "source": [
        "# now we construct our model using the MultiModalPredictor class\n",
        "predictor = MultiModalPredictor(label='target', eval_metric='acc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j2YJz2zmlcIR",
      "metadata": {
        "id": "j2YJz2zmlcIR"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "del(predictor_tab)\n",
        "del(predictor_bc)\n",
        "del(predictor_diabetes)\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4lOV16w7N6S8",
      "metadata": {
        "id": "4lOV16w7N6S8"
      },
      "outputs": [],
      "source": [
        "predictor.fit(drug_reviews_train, time_limit=180)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bmi_dGhiOS8E",
      "metadata": {
        "id": "Bmi_dGhiOS8E"
      },
      "outputs": [],
      "source": [
        "predictor.evaluate(drug_reviews_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H4yab23DRYfr",
      "metadata": {
        "id": "H4yab23DRYfr"
      },
      "outputs": [],
      "source": [
        "predictions = predictor.predict(drug_reviews_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eNG1EBbXRpIv",
      "metadata": {
        "id": "eNG1EBbXRpIv"
      },
      "outputs": [],
      "source": [
        "predictions.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sJiB0D3OTrl9",
      "metadata": {
        "id": "sJiB0D3OTrl9"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(drug_reviews_test['target'], predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E4S4_mn5UsJR",
      "metadata": {
        "id": "E4S4_mn5UsJR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
