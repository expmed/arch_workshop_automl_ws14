{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e7feb5a2",
      "metadata": {
        "id": "e7feb5a2"
      },
      "source": [
        "# WS 12 AutoML with AutoGluon Hands on Module"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Introduction and Installing Package Dependencies\n",
        "In this hands on module, we will see how to simplify the process of training high-quality, optimized machine learning models on sample datasets from UCI Machine Learning repository using the [AutoGluon](https://auto.gluon.ai/stable/index.html) package.\n",
        "We start by installing the `utogluon` and `ucimlrepo` packages with `pip`"
      ],
      "metadata": {
        "id": "gKRURZHj11-T"
      },
      "id": "gKRURZHj11-T"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autogluon\n",
        "!pip install ucimlrepo"
      ],
      "metadata": {
        "id": "Dn8m61eN2BSJ"
      },
      "id": "Dn8m61eN2BSJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset introduction and loading data from UCI ML Repository\n",
        "Now we import pacakges and load in the following three healthcare related datasets from [UCI Machine Learning Repository](https://archive.ics.uci.edu/)\n",
        "\n",
        "\n",
        "*   [Breast Cancer data](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic) from University of Wisconsin\n",
        "*   [Diabetes data](https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) representing ten years of clinical care at 130 US hospitals\n",
        "*  [A Drug reviews](https://archive.ics.uci.edu/dataset/461/drug+review+dataset+druglib+com) dataset providing patient reviews on specific drugs\n",
        "\n",
        "The first two datsets will be used to demonstrate AutoGluon's `TabularPredictor` class and how it enables us to train high-fideltiy ensemble models on data without needing to worry about pre-processing.\n",
        "\n",
        "The third dataset will allow us to explore AutoGluon's `MultiModalPredictor` and how it allows us to train models on plain-text inputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NiMElLZy24Zn"
      },
      "id": "NiMElLZy24Zn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b62dd5b",
      "metadata": {
        "id": "2b62dd5b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ucimlrepo import fetch_ucirepo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here we fix a random seed for reproducibility purposes\n",
        "np.random.seed(913)"
      ],
      "metadata": {
        "id": "YreNspRjV_o1"
      },
      "id": "YreNspRjV_o1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we load in the breast cancer dataset. This dataset contains features that describe the characteristics of cell nuclei present in a digitized image taken from the fine needle aspirate of a breast mass. The labels in the data are binary/two-class, with 'B' representing a benign mass and 'M' representing a malignant mass\n",
        "\n"
      ],
      "metadata": {
        "id": "oa0rkUeg5JDh"
      },
      "id": "oa0rkUeg5JDh"
    },
    {
      "cell_type": "code",
      "source": [
        "# now we load in the breast cancer dataset from UCI\n",
        "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = breast_cancer_wisconsin_diagnostic.data.features\n",
        "y = breast_cancer_wisconsin_diagnostic.data.targets\n"
      ],
      "metadata": {
        "id": "eNnlKxMf43u9"
      },
      "id": "eNnlKxMf43u9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(breast_cancer_wisconsin_diagnostic.variables)"
      ],
      "metadata": {
        "id": "PT0a0HTa6U-B"
      },
      "id": "PT0a0HTa6U-B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "breast_cancer_df = X.assign(\n",
        "    Diagnosis=y\n",
        ")"
      ],
      "metadata": {
        "id": "vuhB5yTh91__"
      },
      "id": "vuhB5yTh91__",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "breast_cancer_df"
      ],
      "metadata": {
        "id": "I9tbb7hb99Ni"
      },
      "id": "I9tbb7hb99Ni",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "breast_cancer_df['Diagnosis'].value_counts()"
      ],
      "metadata": {
        "id": "5eKQ2XK0-MWe"
      },
      "id": "5eKQ2XK0-MWe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we load in the Diabetes dataset. This dataset was constructed with the goal of predicting the early readmission of diabetes patients within 30 days of discharge"
      ],
      "metadata": {
        "id": "VTKvVtP3QstW"
      },
      "id": "VTKvVtP3QstW"
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch dataset\n",
        "diabetes_data = fetch_ucirepo(id=296)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = diabetes_data.data.features\n",
        "y = diabetes_data.data.targets"
      ],
      "metadata": {
        "id": "c3nILN1VRAYE"
      },
      "id": "c3nILN1VRAYE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(diabetes_data.variables)"
      ],
      "metadata": {
        "id": "x4y6ROy8RG9q"
      },
      "id": "x4y6ROy8RG9q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "abIT4UzIEG92"
      },
      "id": "abIT4UzIEG92",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_df = X.assign(\n",
        "    readmitted=y.map(lambda readmit: 0 if readmit == 'NO' else 1) # convert to a binary target\n",
        ")"
      ],
      "metadata": {
        "id": "JKzx07jxRMhH"
      },
      "id": "JKzx07jxRMhH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_df['readmitted'].value_counts()"
      ],
      "metadata": {
        "id": "Oe1LpCMARRBq"
      },
      "id": "Oe1LpCMARRBq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# because this is such a large dataset, we will down-sample this to only include 20% of the dat\n",
        "diabetes_df_downsamp = diabetes_df.sample(frac=0.2)"
      ],
      "metadata": {
        "id": "b1JRJNviEOcg"
      },
      "id": "b1JRJNviEOcg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_df_downsamp['readmitted'].value_counts()"
      ],
      "metadata": {
        "id": "HrIE7YB7EV_s"
      },
      "id": "HrIE7YB7EV_s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we split the two datasets into 80%/20% training/test set splits, so that we can evaluate our tuned models at the very end on unseen test data"
      ],
      "metadata": {
        "id": "PciRQPzH639S"
      },
      "id": "PciRQPzH639S"
    },
    {
      "cell_type": "code",
      "source": [
        "bc_train = breast_cancer_df.sample(frac=0.8)\n",
        "bc_test = breast_cancer_df.drop(bc_train.index)"
      ],
      "metadata": {
        "id": "MDdZzW2OSvmO"
      },
      "id": "MDdZzW2OSvmO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bc_train['Diagnosis'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "N7UATHpSS3Og"
      },
      "id": "N7UATHpSS3Og",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bc_test['Diagnosis'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "wA7YrbzcS5-v"
      },
      "id": "wA7YrbzcS5-v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_train = diabetes_df_downsamp.sample(frac=0.8)\n",
        "diabetes_test = diabetes_df_downsamp.drop(diabetes_train.index)"
      ],
      "metadata": {
        "id": "UhxSy-bTTGJi"
      },
      "id": "UhxSy-bTTGJi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_train['readmitted'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "FNIVuqVDTOBR"
      },
      "id": "FNIVuqVDTOBR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_test['readmitted'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "sCAS2hIwTQcf"
      },
      "id": "sCAS2hIwTQcf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Introduction to AutoGluon Tabular Predictor\n",
        "Now we will see how AutoGluon's `TabularPredictor` class can be used to automatically fit a weighted ensemble on the breast cancer dataset, with automatic K-fold cross validation, bagging, and stacking, and with a large suite of models evaluated for inclusion in the final ensemble"
      ],
      "metadata": {
        "id": "09Bv3a8n8ZsE"
      },
      "id": "09Bv3a8n8ZsE"
    },
    {
      "cell_type": "code",
      "source": [
        "from autogluon.tabular import TabularPredictor"
      ],
      "metadata": {
        "id": "SlqyUWsF8nje"
      },
      "id": "SlqyUWsF8nje",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we do the same with the breast cancer dataset\n",
        "predictor_bc = TabularPredictor( # construct the predictor\n",
        "    label='Diagnosis', eval_metric='roc_auc'\n",
        ").fit( # call the fit method\n",
        "    bc_train,\n",
        "    num_bag_folds=3,\n",
        "    excluded_model_types=['NN_TORCH', 'FASTAI', 'CAT'] # exclude neural nets and CatBoost for faster training\n",
        ")"
      ],
      "metadata": {
        "id": "M2uLil1SFC9d"
      },
      "id": "M2uLil1SFC9d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we evaluate the Breast Cancer model on the test data\n",
        "predictor_bc.evaluate(bc_test)"
      ],
      "metadata": {
        "id": "K3ElcsjwGjfD"
      },
      "id": "K3ElcsjwGjfD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Comparison with Scikit Learn Toy Implementation of Bagging + Stacking\n",
        "We see that our accuracy and precision rival that of the benchmark models listed on the UCI Machine Learning repository page for this dataset.\n",
        "\n",
        "\n",
        "Now, for illustrative purposes, we will take a brief look at how much code it would take to implement a similar *(highly simplified)* k-fold bagging + stacking model ensembling such as what AutoGluon does automatically using Scikit Learn, another popular machine learning framework for Python\n"
      ],
      "metadata": {
        "id": "jD-1EZFFGwsv"
      },
      "id": "jD-1EZFFGwsv"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from xgboost import XGBClassifier\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "7rHfS2Z7V9Tm"
      },
      "id": "7rHfS2Z7V9Tm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# our target labels are text character 'M' and 'B'\n",
        "# Scikit-learn binary classifiers need these to be converted to numeric 1/0\n",
        "bc_train_binary = bc_train.assign(\n",
        "    binary_label=lambda x: x['Diagnosis'].map(lambda diag: 1 if diag == 'M' else 0)\n",
        ").drop(columns='Diagnosis')\n",
        "bc_test_binary = bc_test.assign(\n",
        "    binary_label=lambda x: x['Diagnosis'].map(lambda diag: 1 if diag == 'M' else 0)\n",
        ").drop(columns='Diagnosis')"
      ],
      "metadata": {
        "id": "m1ENeFkuWe0k"
      },
      "id": "m1ENeFkuWe0k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separate features X from targets y\n",
        "X = bc_train_binary.drop(columns=['binary_label'])\n",
        "y = bc_train_binary['binary_label']\n",
        "# initialize the Kfold object for doing kfold cross validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# construct arrays for storing the out of fold prediciotns for the models\n",
        "oof_preds_rf = np.zeros(len(X))\n",
        "oof_preds_xgb = np.zeros(len(X))\n",
        "\n",
        "# save the bagged models in lists\n",
        "\n",
        "# specifiy the classifiers that will be in each layer\n",
        "layers = [RandomForestClassifier, XGBClassifier]\n",
        "layer_preds = [oof_preds_rf, oof_preds_xgb]\n",
        "layer_bags = [list(), list()]\n",
        "# loop over our layers\n",
        "for i, layer in enumerate(layers):\n",
        "  print(f\"Performing k-fold cross validation at layer {i} with {layer}\")\n",
        "  # do the K-fold cross validation loop\n",
        "  for train_idx, val_idx in tqdm(kf.split(X), total=5):\n",
        "      # split inputs and outputs into training and validation\n",
        "      X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "      y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "      # if we are in a layer past the first layer, the inputs need to include\n",
        "      # the predictions from the prior layer\n",
        "      if i > 0:\n",
        "        X_train = np.column_stack([\n",
        "            X_train.to_numpy(),\n",
        "            layer_preds[i-1][train_idx] # include preds from prior layer\n",
        "        ])\n",
        "        X_val = np.column_stack([\n",
        "            X_val.to_numpy(),\n",
        "            layer_preds[i-1][val_idx]\n",
        "        ])\n",
        "\n",
        "      # fit a model from the given layer on the training fold\n",
        "      model = layer()\n",
        "      model.fit(X_train, y_train)\n",
        "      # evaluate it on the validation fold and save oof predictions\n",
        "      layer_preds[i][val_idx] = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "      # save the model in our layer bag\n",
        "      layer_bags[i].append(model)\n",
        "\n",
        "# final meta model: Weighted ensemble of the predictions from the prior layers\n",
        "meta_features = np.column_stack(layer_preds)\n",
        "\n",
        "meta_model = LogisticRegression()\n",
        "meta_model.fit(meta_features, y)"
      ],
      "metadata": {
        "id": "tgH2TZ5PWHfD"
      },
      "id": "tgH2TZ5PWHfD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to evaluate this custom Meta model on the unseen test data, we need to\n",
        "\n",
        "\n",
        "1.   Get 5 separate sets of predictions from each Random Forest model in the first layer\n",
        "2.   Get 5 separate sets of predictions from each XGBoost model in the second layer, appending the predictions from the first layer models as feature inputs to the second layer models\n",
        "3. Average the predicions made at each layer into a single set of predictions per layer\n",
        "4. Append these two sets of predictions together into the final features to feed to the Meta Model (Logistic Regression)\n",
        "5. Get the final predictions from the Meta Model\n",
        "\n"
      ],
      "metadata": {
        "id": "74LPiyADbaz7"
      },
      "id": "74LPiyADbaz7"
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = bc_test_binary.drop(columns=['binary_label'])\n",
        "y_test = bc_test_binary['binary_label']\n",
        "\n",
        "layer_preds_test = []\n",
        "\n",
        "# steps 1 and 2 - Getting separate predictions from models in each layer\n",
        "for i in range(len(layer_bags)):\n",
        "  # concatenate predictions from prior layer to features if needed\n",
        "  if i > 0:\n",
        "    X_test_stacked = np.column_stack([X_test.to_numpy(), layer_preds_test[i-1]])\n",
        "  else:\n",
        "    X_test_stacked = X_test\n",
        "\n",
        "  # step 3 - compute average predictions across all models in the layer\n",
        "  current_layer_preds = sum([\n",
        "      model.predict_proba(X_test_stacked)[:, 1]\n",
        "      for model in layer_bags[i]\n",
        "  ]) / len(layer_bags[i])\n",
        "\n",
        "  # save the predictions for the layer\n",
        "  layer_preds_test.append(current_layer_preds)\n",
        "\n",
        "# Step 4 - concatenate layer predicitons\n",
        "meta_features_test = np.column_stack(layer_preds_test)\n",
        "\n",
        "# Step 5 - get final predictions\n",
        "final_predictions = meta_model.predict(meta_features_test)\n",
        "\n",
        "# get the accuracy and f1 score\n",
        "print(f\"F1: {f1_score(y_test, final_predictions)}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, final_predictions)}\")\n"
      ],
      "metadata": {
        "id": "m5xTfZBVbBE8"
      },
      "id": "m5xTfZBVbBE8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Applying AutoGluon to a more complex dataset and threshold tuning\n",
        "We can see that a considerable amount of code, understanding, and index manipulation was required in order to get an ensemble implementation using Scikit-learn that approaches the functionality which AutoGluon provides in only 3-4 lines of code. The toy implementation above also does not perform any greedy model weighting or pruning of models, and only incorporated two model families in the ensemble.\n",
        "\n",
        "Here we train another TabularPredictor on the Diabetes readmission data, to assess how well it scales to larger datasets with heterogeneous input types. Here we will also see how one can tune the decision threshold of a binary classifier with AutoGluon in order to try and achieve better Recall and performance on the positive class"
      ],
      "metadata": {
        "id": "i_3rXJganwrt"
      },
      "id": "i_3rXJganwrt"
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_train"
      ],
      "metadata": {
        "id": "PSoqCnOioqqx"
      },
      "id": "PSoqCnOioqqx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor_diabetes = TabularPredictor( # construct the predictor\n",
        "    label='readmitted', eval_metric='roc_auc'\n",
        ").fit( # call the fit method\n",
        "    train_data=diabetes_train,\n",
        "    excluded_model_types=['NN_TORCH', 'FASTAI', 'CAT'], # excluding neural nets for faster training\n",
        ")"
      ],
      "metadata": {
        "id": "V_lHDvBJo8v9"
      },
      "id": "V_lHDvBJo8v9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor_diabetes.evaluate(diabetes_test)"
      ],
      "metadata": {
        "id": "hyuN8Cl1pbaS"
      },
      "id": "hyuN8Cl1pbaS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The recall of 0.54 tells us that only ~54% of patients who were readmitted to a hospital were successfully classified as such by our model. If we want to tune our classification threshold to achieve a better recall/sensitivity/TPR (true-positive rate), as we often do when working with medical data and developing tests for the presence of risks/conditions, AutoGluon makes this very easy."
      ],
      "metadata": {
        "id": "NzoSVhKyKPPd"
      },
      "id": "NzoSVhKyKPPd"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, classification_report"
      ],
      "metadata": {
        "id": "kDdTjYFE0ptL"
      },
      "id": "kDdTjYFE0ptL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_preds = predictor_diabetes.predict(diabetes_test)"
      ],
      "metadata": {
        "id": "PXnNRZAEK3Mu"
      },
      "id": "PXnNRZAEK3Mu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay.from_predictions(diabetes_test['readmitted'], diabetes_preds)"
      ],
      "metadata": {
        "id": "_tVwXKJvK5fR"
      },
      "id": "_tVwXKJvK5fR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we calibrate the decision threshold of our model, using the F1 score as the calibration metric\n",
        "threshold = predictor_diabetes.calibrate_decision_threshold(metric='f1')"
      ],
      "metadata": {
        "id": "qTXQon333ODy"
      },
      "id": "qTXQon333ODy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conceptual Question - Why did we calibrate the decision threshold to optimize the [F1 score](https://en.wikipedia.org/wiki/F-score) (the harmonic mean of precision and recall) if our goal was to achieve higher recall? Why not just use recall as the calibration metric directly?"
      ],
      "metadata": {
        "id": "VcgRghAoJ4jJ"
      },
      "id": "VcgRghAoJ4jJ"
    },
    {
      "cell_type": "code",
      "source": [
        "predictor_diabetes.set_decision_threshold(threshold)"
      ],
      "metadata": {
        "id": "VQ2tMTzZ3ZNm"
      },
      "id": "VQ2tMTzZ3ZNm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor_diabetes.evaluate(diabetes_test)"
      ],
      "metadata": {
        "id": "C3B1v1HJ3e0q"
      },
      "id": "C3B1v1HJ3e0q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_preds = predictor_diabetes.predict(diabetes_test)"
      ],
      "metadata": {
        "id": "dD7xNZvbLI6A"
      },
      "id": "dD7xNZvbLI6A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay.from_predictions(diabetes_test['readmitted'], diabetes_preds)"
      ],
      "metadata": {
        "id": "D-luEmxALJl9"
      },
      "id": "D-luEmxALJl9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 6. Tackling a Text Classification Problem with AutoGluon Tabular and MultiModal\n",
        "Now that we will see how AutoGluon can also seamlessly tackle plain-text columns, such as written reviews, using both the `TabularPredictor` class, as well as the more sophisticated `MultiModalPredictor` which trains a full transformer neural network to make predictions on text-based data."
      ],
      "metadata": {
        "id": "fzxUGnHpLq6v"
      },
      "id": "fzxUGnHpLq6v"
    },
    {
      "cell_type": "code",
      "source": [
        "from autogluon.multimodal import MultiModalPredictor"
      ],
      "metadata": {
        "id": "-FbgZFX4ME97"
      },
      "id": "-FbgZFX4ME97",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we load in the Drug Reviews dataset from UCI Machine learning repository\n",
        "drug_reviews_druglib_com = fetch_ucirepo(id=461)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = drug_reviews_druglib_com.data.features"
      ],
      "metadata": {
        "id": "MzzX1RXVMIOs"
      },
      "id": "MzzX1RXVMIOs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(drug_reviews_druglib_com.variables)"
      ],
      "metadata": {
        "id": "agG-ZW5kMZ2G"
      },
      "id": "agG-ZW5kMZ2G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "0x5UliUVNnyS"
      },
      "id": "0x5UliUVNnyS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct a 3-class label based on the numeric rating from 1-10\n",
        "targets = X['rating'].map(lambda rating: 'positive' if rating >= 7 else 'neutral' if rating >= 4 else 'negative')"
      ],
      "metadata": {
        "id": "QhrSSYhwNV7A"
      },
      "id": "QhrSSYhwNV7A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_reviews_df = X[['benefitsReview', 'sideEffectsReview', 'commentsReview']].assign(\n",
        "    target=targets\n",
        ")"
      ],
      "metadata": {
        "id": "QkhjhDZHMh44"
      },
      "id": "QkhjhDZHMh44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_reviews_df"
      ],
      "metadata": {
        "id": "EkDaaqnINkSH"
      },
      "id": "EkDaaqnINkSH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_reviews_df['target'].value_counts()"
      ],
      "metadata": {
        "id": "TfRMVoM5Nzv6"
      },
      "id": "TfRMVoM5Nzv6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will downsample the three classes to achieve a smaller dataset for demonstration purposes\n",
        "drug_reviews_downsamp = pd.concat([\n",
        "    drug_reviews_df.query('target == \"positive\"').sample(n=400),\n",
        "    drug_reviews_df.query('target == \"neutral\"').sample(n=400),\n",
        "    drug_reviews_df.query('target == \"negative\"').sample(n=400)\n",
        "])"
      ],
      "metadata": {
        "id": "bqfaVdGCRv4S"
      },
      "id": "bqfaVdGCRv4S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_reviews_train = drug_reviews_downsamp.sample(frac=0.8)\n",
        "drug_reviews_test = drug_reviews_downsamp.drop(drug_reviews_train.index)"
      ],
      "metadata": {
        "id": "piD3hOA9N9LI"
      },
      "id": "piD3hOA9N9LI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_reviews_train['target'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "jSmiNc8ROGkt"
      },
      "id": "jSmiNc8ROGkt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_reviews_test['target'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "BrV0BoeZOJ7R"
      },
      "id": "BrV0BoeZOJ7R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first we fit a TabularPredictor to the dataset to see how it performs\n",
        "predictor_tab = TabularPredictor(label='target', eval_metric='acc')"
      ],
      "metadata": {
        "id": "y7drEuRnkNpX"
      },
      "id": "y7drEuRnkNpX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor_tab.fit(drug_reviews_train)"
      ],
      "metadata": {
        "id": "3JU2KY68kVSr"
      },
      "id": "3JU2KY68kVSr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we evaluate the model's performance on the test data\n",
        "predictor_tab.evaluate(drug_reviews_test)"
      ],
      "metadata": {
        "id": "S98glnhPku_c"
      },
      "id": "S98glnhPku_c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay.from_predictions(drug_reviews_test['target'], predictor_tab.predict(drug_reviews_test))"
      ],
      "metadata": {
        "id": "tRKokVu8k_2f"
      },
      "id": "tRKokVu8k_2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(drug_reviews_test['target'], predictor_tab.predict(drug_reviews_test)))"
      ],
      "metadata": {
        "id": "pC6M7ckqlMSD"
      },
      "id": "pC6M7ckqlMSD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we construct our model using the MultiModalPredictor class\n",
        "predictor = MultiModalPredictor(label='target', eval_metric='acc')"
      ],
      "metadata": {
        "id": "pG79RvWoMjqf"
      },
      "id": "pG79RvWoMjqf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "del(predictor_tab)\n",
        "del(predictor_bc)\n",
        "del(predictor_diabetes)\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "j2YJz2zmlcIR"
      },
      "id": "j2YJz2zmlcIR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.fit(drug_reviews_train, time_limit=180)"
      ],
      "metadata": {
        "id": "4lOV16w7N6S8"
      },
      "id": "4lOV16w7N6S8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.evaluate(drug_reviews_test)"
      ],
      "metadata": {
        "id": "Bmi_dGhiOS8E"
      },
      "id": "Bmi_dGhiOS8E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predictor.predict(drug_reviews_test)"
      ],
      "metadata": {
        "id": "H4yab23DRYfr"
      },
      "id": "H4yab23DRYfr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.value_counts()"
      ],
      "metadata": {
        "id": "eNG1EBbXRpIv"
      },
      "id": "eNG1EBbXRpIv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay.from_predictions(drug_reviews_test['target'], predictions)"
      ],
      "metadata": {
        "id": "sJiB0D3OTrl9"
      },
      "id": "sJiB0D3OTrl9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E4S4_mn5UsJR"
      },
      "id": "E4S4_mn5UsJR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}